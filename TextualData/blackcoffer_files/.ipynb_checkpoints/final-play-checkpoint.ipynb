{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Creating a function to remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>There are 3 ways we remove our stop words :</b>\n",
    "\n",
    "1. Considering all the words that are not in master dictionary as stop words\n",
    "2. Link to a website containing STOPWORDS files, can be used to a list of STOPWORDS\n",
    "   which we will find out that, it has 12K around STOPWORDS\n",
    "3. Using NLTK stopwords(it might not be a much of a preferred way because NLTK dosesn't have that many words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>USING MASTER DICTIONARY</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict_ini = pd.read_excel('LoughranMcDonald_MasterDictionary_2018.xlsx')\n",
    "\n",
    "md = master_dict_ini.copy()\n",
    "\n",
    "#md\n",
    "\n",
    "md['Word'] = md['Word'].apply(lambda x: str(x).lower())\n",
    "words_to_keep = list(md['Word'])\n",
    "\n",
    "words_to_keep.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(words_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OR: STOPWORDS FROM THE GIVEN WEBSITE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "stop_word_file_path = 'STOP-WORDS/'\n",
    "\n",
    "list_of_files = os.listdir(stop_word_file_path)\n",
    "\n",
    "#2\n",
    "for i in list_of_files:\n",
    "    \n",
    "    with open(os.path.join(stop_word_file_path, i), 'r') as words :\n",
    "        \n",
    "        content = words.read()\n",
    "        \n",
    "        with open('stop_words.txt', 'a+') as stop_words :\n",
    "            \n",
    "            stop_words.write(content + '\\n')\n",
    "            \n",
    "            \n",
    "#3\n",
    "with open('stop_words.txt', 'r') as stop_words :\n",
    "            \n",
    "            list_of_stop_words = stop_words.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some modification req in stop_words_list\n",
    "\n",
    "for i in range(len(list_of_stop_words)) :\n",
    "    \n",
    "    list_of_stop_words[i] = list_of_stop_words[i].replace('\\n', \"\").lower()\n",
    "\n",
    "\n",
    "list_of_stop_words = list(set(list_of_stop_words))\n",
    "\n",
    "list_of_stop_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#list_of_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OR: STOPWORDS FROM NLTK MODULE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>THE FUNCTION TO FILTER OUT STOPWORDS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MODE parameter</b> states which way you want to find stop words\n",
    "\n",
    "- <b>MODE = 0</b> : stopwords from website\n",
    "- <b>MODE = 1</b> : stopwords from nltk library\n",
    "- <b>MODE = 2</b> : using master dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(content, MODE=0):\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    if MODE == 0 :\n",
    "        \n",
    "        for i in content:\n",
    "\n",
    "            if i not in list_of_stop_words:\n",
    "\n",
    "                filtered_words.append(i)\n",
    "\n",
    "        return filtered_words\n",
    "    \n",
    "    if MODE == 1 :\n",
    "        \n",
    "        for i in content:\n",
    "\n",
    "            if i not in stop_words:\n",
    "\n",
    "                filtered_words.append(i)\n",
    "                \n",
    "        return filtered_words\n",
    "    \n",
    "    if MODE == 2 :\n",
    "        \n",
    "        for i in content:\n",
    "            \n",
    "            if i in words_to_keep:\n",
    "                \n",
    "                filtered_words.append(i)\n",
    "                \n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 : Extracting the (textual data + related variables) we need, from each financial report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A - PREPARING DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda xlrd --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_report_ini_data = pd.read_excel('cik_list.xlsx') #original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data = financial_report_ini_data.copy() #copying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data['SECFNAME'] = 'https://www.sec.gov/Archives/' + fin_rep_data['SECFNAME'] #modifying the column F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fin_rep_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B - GETTING UNCERTAINITY AND CONSTRAINING WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting uncertain and constraining words \n",
    "\n",
    "uw_initial = pd.read_excel('uncertainty_dictionary.xlsx')\n",
    "cons_initial = pd.read_excel('constraining_dictionary.xlsx')\n",
    "\n",
    "uncertainity_words = uw_initial.copy()\n",
    "constraining_words = cons_initial.copy()\n",
    "\n",
    "#constraining_words\n",
    "\n",
    "uncertainity_words['Word'] = uncertainity_words['Word'].apply(lambda x: str(x).lower())\n",
    "constraining_words['Word'] = constraining_words['Word'].apply(lambda x: str(x).lower())\n",
    "\n",
    "uncertainity_words = list(uncertainity_words['Word'])\n",
    "constraining_words = list(constraining_words['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainity_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraining_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - modifying FDATE column's datastructure, for better use of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(fin_rep_data.loc[0, 'FDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_rep_data['FDATE'] = fin_rep_data['FDATE'].apply(lambda x : datetime.date(x.year,x.month,x.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fin_rep_data.loc[0, 'FDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_rep_data.loc[0, 'FDATE'].year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D - EXTRACTING SECTION WISE INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <b>EXTRACTING CONTENT FROM URLS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fin_rep_data.index:\n",
    "    \n",
    "    url[i] = fin_rep_data.loc[i, 'SECFNAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <b>SECTIONS TO LOOK FOR</b> : \n",
    "management's discussion and analysis, quantitative and qualitative disclosures about market risk, risk factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    #qqdmr\n",
    "    \n",
    "    qqdmr_text = re.findall(r'(\\sitem\\s\\d\\w?\\.\\squantitative\\sand\\squalitative\\sdisclosures\\sabout\\smarket\\srisk\\s)(.*)(item\\s\\d\\w?\\.)',text)\n",
    "    \n",
    "    if len(qqdmr_text) == 0:\n",
    "        print('qqdmr case 2')\n",
    "        qqdmr_text = re.findall(r'(\\sitem\\s\\d\\w?\\.\\squantitative\\sand\\squalitative\\sdisclosures\\sabout\\smarket\\srisk\\s)(.*)(-----end privacy-enhanced message-----)',text)\n",
    "        \n",
    "        if len(qqdmr_text) == 0:\n",
    "            qqdmr_text = 'None'\n",
    "        \n",
    "    #mda\n",
    "    \n",
    "    mda_text = re.findall(r'(\\sitem\\s\\d\\w?\\.\\smanagement\\ss\\sdiscussion\\sand\\sanalysis\\s)(.*)(item\\s\\d\\w?\\.)', text)\n",
    "    \n",
    "    if len(mda_text) == 0:\n",
    "        print('mda case 2')\n",
    "        mda_text = re.findall(r'(\\sitem\\s\\d\\w?\\.\\smanagement\\ss\\sdiscussion\\sand\\sanalysis\\s)(.*)(-----end privacy-enhanced message-----)', text)\n",
    "        \n",
    "        if len(mda_text) == 0: \n",
    "            mda_text = 'None'\n",
    "        \n",
    "    #rf\n",
    "    \n",
    "    rf_text = re.findall(r'(\\sitem\\s\\d\\w?\\.\\srisk\\sfactors\\s)(.*)(item\\s\\d\\w?\\.)', text)\n",
    "    \n",
    "    if len(rf_text) == 0:\n",
    "        print('rf case 2')\n",
    "        rf_text = re.findall(r'(\\sitem\\s\\d\\w?\\.\\srisk\\sfactors\\s)(.*)(-----end privacy-enhanced message-----)', text)\n",
    "        \n",
    "        if len(rf_text) == 0:\n",
    "            rf_text = 'None'\n",
    "        \n",
    "    return (mda_text, qqdmr_text, rf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>FUNCTIONS FOR MODIFICATIONS & GETTING VARIABLES' VALUES IN-HAND:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <b>`modify_file_content`</b> \n",
    "is the function that remove unneccessary characters in whole file, and returns content in the form of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_file_content(l):\n",
    "    \n",
    "    #l will the string of content in file and function returns a full content in form of single string\n",
    "    \n",
    "    l = re.sub(r'<.*>|\\n|\\t|(&nbsp;)|(&#\\d+)|;', ' ', l)\n",
    "    l = re.sub(r'(\\\\x\\d\\d)|(\\\\x\\w\\d)|(\\\\x\\d\\ds)','', l)\n",
    "    l = re.sub(r'(\\\\x92s)', '', l)\n",
    "    l = re.sub('\\s+', ' ', l)\n",
    "    \n",
    "#     for i in l:\n",
    "#         i.replace('\\n', ' ')\n",
    "#         i.replace('\\\\', '')\n",
    "#         i.replace('\\t', ' ')\n",
    "#         i.replace(\"\\'\", '')\n",
    "#         i.replace('\\xa0', '')\n",
    "#         i.replace('\\x', '')\n",
    "    \n",
    "    #new_str = ' '.join(l)\n",
    "    \n",
    "    #return new_str\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <b>`clean`</b> \n",
    "is used on extracted out sections rather than whole file. It first removes more of unneccessary characters and then removes stopwords. It returns list of remaining words that are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(specific_section_text):\n",
    "    \n",
    "    actual_content = specific_section_text\n",
    "    \n",
    "    # 1. UNNECCESSARY CHARACTERS REMOVAL\n",
    "    \n",
    "    # actual_content = re.sub(r\"(<.*>)\",r\" \",actual_content).lower()\n",
    "    \n",
    "    actual_content = re.sub(\"[^a-zA-Z]\",\" \",actual_content).lower()\n",
    "\n",
    "    actual_content = re.sub(r'\\s+', r' ', actual_content).lower() \n",
    "    \n",
    "    # 3. TOKENIZE\n",
    "    \n",
    "    actual_content = word_tokenize(actual_content)\n",
    "    \n",
    "    # 2. REMOVING STOP WORDS\n",
    "    \n",
    "    # actual_content = list(set(actual_content.split()))\n",
    "        \n",
    "    actual_content = remove_stop_words(actual_content, MODE=0)\n",
    "    \n",
    "    return actual_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <b>`section_content_info`</b> \n",
    "is the function that extracts out particular sections from whole file, based on regex pattern provided and applies cleaning with `clean` function. It return number of sentences and useful words from the excerpt. It returns None if that particular section is not found in particular file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_content_info(content_string):\n",
    "    \n",
    "    #temp_list = re.findall(pattern, content_string, re.M)\n",
    "    \n",
    "    if len(temp_list) != 0 :\n",
    "\n",
    "        actual_content = content_string\n",
    "\n",
    "        number_of_sentences = len(sent_tokenize(actual_content))\n",
    "        \n",
    "        actual_content = clean(actual_content)\n",
    "        \n",
    "        # NLTK TOKENIZER\n",
    "\n",
    "        #    IT CAN BE NOTED THAT OUR TEXT STRING IS TOKENIZED UPTILL NOW\n",
    "        #    WE DON'T NEED NLTK TOKENIZER, BUT STILL, JUST IN CASE WE\n",
    "        #    HAVE MISSED SOMETHING, NLTK TOKENIZER WILL HANDLE THAT !\n",
    "\n",
    "        actual_content = ' '.join(actual_content)\n",
    "\n",
    "        final_words = word_tokenize(actual_content)\n",
    "\n",
    "        return [[final_words], number_of_sentences]\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. <b>`analysis_whole_report`</b> \n",
    "is the function that finds the number of constraining words for a given file, which needs to be added as last variable in our final output data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_whole_report(content):\n",
    "    \n",
    "    new_content = clean(content)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for i in new_content :\n",
    "        \n",
    "        if i in constraining_words :\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>THE MAIN LOOP</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_bs(html):\n",
    "    tree = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    body = tree.body\n",
    "    if body is None:\n",
    "        return None\n",
    "\n",
    "    for tag in body.select('script'):\n",
    "        tag.decompose()\n",
    "    for tag in body.select('style'):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = body.get_text(separator='\\n')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = urllib.request.urlopen('https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002278.txt')\n",
    "# content = response.read().decode('utf8')\n",
    "# content = get_text_bs(content)\n",
    "# content = content.lower()\n",
    "# content = re.sub(r'<.*>|\\n|\\t|(&nbsp;)|(&#\\d+)|;', ' ', content)\n",
    "# content = re.sub('\\s+', ' ', content)\n",
    "# a,b,c = extract_sections(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = {}\n",
    "count=0\n",
    "for i in url:\n",
    "    count+=1\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url[i])\n",
    "        \n",
    "    content = response.read().decode('utf8')\n",
    "    content = get_text_bs(content)\n",
    "    content = content.lower()\n",
    "    content = re.sub(r'<.*>|\\n|\\t|(&nbsp;)|(&#\\d+)|;', ' ', content)\n",
    "    content = re.sub('\\s+', ' ', content)\n",
    "    a,b,c = extract_sections(content)\n",
    "    file[count] = {'a':a, 'b':b, 'c':c,'url':url[i]}\n",
    "    constraining_words_whole_report[c] = analysis_whole_report(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - <b>OTHER SCORES</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying the index of master dictionary\n",
    "\n",
    "md = md.set_index(['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#md.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE FUNCTION CALCULATING SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(words, number_of_sentences):\n",
    "    \n",
    "    negative_score = 0\n",
    "    \n",
    "    positive_score = 0\n",
    "    \n",
    "    complex_word_count = 0\n",
    "    \n",
    "    word_count = len(words)\n",
    "    \n",
    "    word_length = 0\n",
    "    \n",
    "    uncertainty_score = 0\n",
    "    \n",
    "    constraining_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        word_length += len(word)\n",
    "        \n",
    "        if md.loc[word, 'Syllables'] >2 :\n",
    "            \n",
    "            complex_word_count +=1\n",
    "        \n",
    "        if md.loc[word, 'Positive'] != 0:\n",
    "            \n",
    "            positive_score += 1\n",
    "        \n",
    "        if md.loc[word, 'Negative'] != 0:\n",
    "            \n",
    "            negative_score += 1\n",
    "    \n",
    "        if word in uncertainity_words:\n",
    "            \n",
    "            uncertainty_score += 1\n",
    "            \n",
    "        if word in constraining_words:\n",
    "            \n",
    "            constraining_score += 1\n",
    "            \n",
    "    polarity_score = (positive_score - negative_score)/((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    subjectivity_score = (positive_score + negative_score)/(len(words) + 0.000001)\n",
    "\n",
    "    average_sentence_length = len(words)/number_of_sentences\n",
    "    \n",
    "    percentage_of_complex_words = complex_word_count / len(words)\n",
    "    \n",
    "    fog_index = (0.4)*(average_sentence_length + percentage_of_complex_words)\n",
    "    \n",
    "    average_word_length = word_length / len(words)\n",
    "    \n",
    "    positive_word_proportion = positive_score / word_count\n",
    "    \n",
    "    negative_word_proportion = negative_score / word_count\n",
    "    \n",
    "    uncertainty_word_proportion = uncertainty_score / word_count\n",
    "    \n",
    "    constraining_word_proportion = constraining_score / words_count\n",
    "    \n",
    "    return (positive_score, \n",
    "            negative_score, \n",
    "            average_sentence_length, \n",
    "            percentage_of_complex_words, \n",
    "            fog_index, \n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            uncertainty_score,\n",
    "            constraining_score,\n",
    "            positive_word_proportion,\n",
    "            negative_word_proportion,\n",
    "            uncertainty_word_proportion,\n",
    "            constraining_word_proportion\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "ref_df = pd.read_excel('cik_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'SECFNAME']] = ref_df[['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'SECFNAME']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(positive_score, \n",
    "            negative_score, \n",
    "            average_sentence_length, \n",
    "            percentage_of_complex_words, \n",
    "            fog_index, \n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            uncertainty_score,\n",
    "            constraining_score,\n",
    "            positive_word_proportion,\n",
    "            negative_word_proportion,\n",
    "            uncertainty_word_proportion,\n",
    "            constraining_word_proportion\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in final_df.columns[6:] :\n",
    "    exec(var + \"= pd.Series()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_calculation(pos_score, neg_score):\n",
    "    \n",
    "    return (pos_score-neg_score) / (0.000001 + (pos_score+neg_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,1+len(url)):\n",
    "        #print(file[i+1])\n",
    "        \n",
    "        mda_content, number_of_sentences = section_content_info(file[i]['a'])\n",
    "        \n",
    "        mda_positive_score[i], mda_negative_score[i], mda_average_sentence_length[i],\\ \n",
    "        mda_percentage_of_complex_words[i], mda_fog_index[i], mda_complex_word_count[i], mda_word_count[i],\\\n",
    "        mda_uncertainty_score[i], mda_constraining_score[i], mda_positive_word_proportion[i],\\\n",
    "        mda_negative_word_proportion[i], mda_uncertainty_word_proportion[i], mda_constraining_word_proportion[i]\\\n",
    "        = mda_content, number_of_sentences\n",
    "        \n",
    "        qqdmr_content, number_of_sentences = section_content_info(file[i]['b'])\n",
    "        \n",
    "        qqdmr_positive_score[i], qqdmr_negative_score[i], qqdmr_average_sentence_length[i],\\ \n",
    "        qqdmr_percentage_of_complex_words[i], qqdmr_fog_index[i], qqdmr_complex_word_count[i], qqdmr_word_count[i],\\\n",
    "        qqdmr_uncertainty_score[i], qqdmr_constraining_score[i], qqdmr_positive_word_proportion[i],\\\n",
    "        qqdmr_negative_word_proportion[i], qqdmr_uncertainty_word_proportion[i], qqdmr_constraining_word_proportion[i]\\\n",
    "        = qqdmr_content, number_of_sentences\n",
    "        \n",
    "        rf_content, number_of_sentences = section_content_info(file[i]['c'])\n",
    "        \n",
    "        rf_positive_score[i], rf_negative_score[i], rf_average_sentence_length[i],\\ \n",
    "        rf_percentage_of_complex_words[i], rf_fog_index[i], rf_complex_word_count[i], rf_word_count[i],\\\n",
    "        rf_uncertainty_score[i], rf_constraining_score[i], rf_positive_word_proportion[i],\\\n",
    "        rf_negative_word_proportion[i], rf_uncertainty_word_proportion[i], rf_constraining_word_proportion[i]\\\n",
    "        = rf_content, number_of_sentences\n",
    "        \n",
    "        mda_polarity_score[i] = polarity_calculation(mda_positive_score[i], mda_negative_scorea[i])\n",
    "        qqdmr_polarity_score[i] = polarity_calculation(qqdmr_positive_score[i], qqdmr_negative_scorea[i])\n",
    "        rf_polarity_score[i] = polarity_calculation(rf_positive_score[i], rf_negative_scorea[i])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL DATA STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in final_df.columns[6:] :\n",
    "    exec(\"final_df['var'] = var\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
